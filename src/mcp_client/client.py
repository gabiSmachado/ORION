import asyncio
import json
from typing import Optional
from contextlib import AsyncExitStack
from openai import OpenAI
from anthropic import Anthropic
from google import genai
from google.genai import types
from mcp import ClientSession
from mcp.client.sse import sse_client
import logging 
import requests
import utils.resolver as resolver
from pathlib import Path
from utils.results_file import save_results


class MCPClient:
    def __init__(self, logger: logging, rapp: str, file_path: Path):
        self.session: Optional[ClientSession] = None
        self.exit_stack = AsyncExitStack()
        self.tools = []
        self.messages = []
        self.genai_messages = []
        self.logger = logger
        self.rapp = rapp
        self.instructions = None
        self.llm = None
        self.llm_model = None
        self.result_file_path = file_path
        self.results_log = None
        
    async def set_llm(self, llm_model:str, api_key:str):
        try:
            self.logger.info(f"Setting llm model: {llm_model}")
            
            self.llm_model = llm_model
            mcp_tools = await self.get_mcp_tools()
            
            if llm_model == "openai":
                self.llm =  OpenAI(api_key=api_key)       
                self.tools = [
                    {
                        "type": "function",
                        "name": tool.name,
                        "description": tool.description,
                        "parameters": tool.inputSchema,
                    }
                    for tool in mcp_tools
                ]
            elif llm_model == "anthropic":
                self.llm =  Anthropic(api_key=api_key)
                self.tools = [
                    {
                        "name": tool.name,
                        "description": tool.description,
                        "input_schema": tool.inputSchema,
                    }
                    for tool in mcp_tools
                ]
                
            elif llm_model == "gemini":
                self.llm =  genai.Client(api_key=api_key)
                tools_declaration = []

                for tool in mcp_tools:
                    clean_parameters = resolver.resolve_genai_schema(tool.inputSchema)
                    print(type( tool.inputSchema))
                    tools_declaration.append({
                        "name": tool.name,
                        "description": tool.description,
                        "parameters": clean_parameters
                    })
                tools = types.Tool(function_declarations=tools_declaration)
                self.tools = types.GenerateContentConfig(tools=[tools])

            self.messages.insert(0, {
                    "role": "user", 
                    "content": (
                        """
                        # Identity
                        You are an assistant that manages network slice reservations for developers via MCP tool calls.
                        # Instructions
                        * If the user explicitly requests the minimum throughput (downstream or upstream), reply exactly "Minimum values are not supported" and take no further action.
                        * The number of devices (UEs), service time, and service area are not required values.
                        * The number of devices does not have a defined maximum value, as it depends on the value entered by the user.
                        * Consider the number of UEs/devices only when defined by the user.
                        * Ask for throughput/downstream/upstream units only when the user provides a value without any unit. If the value already includes a unit (accept case-insensitive variants such as bps, kbps, Mbps, Gbps, Tbps, Mb/s, megabits per second, etc.), proceed without asking again and reuse that unit.
                        * Service time and area stay null unless the user specifies them.
                        * If a latency/delay budget is given without a unit, assume "Milliseconds".
                        * Don't treat mentions of cost or budget as guidance for choosing conservative values, only populate fields that the user requested.
                        * All fields stay null unless the user specifies them, never fabricate values.
                        * Only reply with plain text when no tool matches.
                        * The response must be **exclusively in the JSON format generated by the tool**, **without additional comments, explanations, or phrases outside the standard format**.
                        """
                    )
                }
            )

            # Seed GenAI message stream with the same instruction text
            try:
                self.genai_messages.insert(0, types.Content(
                    role="user",
                    parts=[types.Part(text=self.messages[0]["content"])],
                ))
            except Exception:
                # If any shape mismatch occurs, skip seeding without breaking other models
                pass

            self.instructions=(
                            "Identify the slice type and add it to the tool's output."
                            "Do not wrap the JSON in code fences or prepend ```json and return it as raw text."
                            "The JSON must have the same values as the tool output, but with a new value. Example: sliceType:'eMBB'"
                            "Types of slice:"
                            "eMBB: Enhanced Mobile Broadband, focuses on delivering high data rates, strong capacity, and consistent user experience."
                            "Designed for high-bandwidth applications like streaming, gaming, and virtual reality."
                            "uRRLC: Ultra-Reliable Low Latency Communication, supports critical applications that need a high degree of reliability "
                            "and very low latency, like drone control, autonomous vehicles, industrial automation, and remote robotic surgery."
                            "mMTC: Massive Machine Type Communications, intended to connect a vast number of devices, such as those in the "
                            "Internet of Things (IoT), which requires low data rates."
                            "Used in applications where many devices are interconnected for purposes like smart sensors, connected homes, building smart cities, or monitoring environments."
                        )
            self.logger.info("LLM configuration completed successfully.")
        except Exception as e:
            self.logger.error(f"Error seting LLM model: {e}")
            raise
        
    async def connect_to_server(self, server_url: str):
        """Connect to an MCP server.

        host/port/path are passed explicitly (taken from config in main) so we don't
        silently use a hardâ€‘coded 127.0.0.1 which breaks in Kubernetes.
        """
        self.logger.info(f"Attempting to connect to server at {server_url}.")
        try:
            result = await self.exit_stack.enter_async_context(
                sse_client(server_url)
            )
            if isinstance(result, (tuple, list)):
                if len(result) < 2:
                    raise RuntimeError("streamablehttp_client returned fewer than 2 elements; cannot get read/write streams")
                self.read_stream, self.write_stream = result[0], result[1]
            else:
                self.read_stream = getattr(result, "read_stream", getattr(result, "read", None))
                self.write_stream = getattr(result, "write_stream", getattr(result, "write", None))
                if self.read_stream is None or self.write_stream is None:
                    raise RuntimeError("Unable to locate read/write streams on streamablehttp_client result")

            self.session = await self.exit_stack.enter_async_context(
                ClientSession(self.read_stream, self.write_stream)
            )

            try:
                await self.session.initialize()
            except asyncio.CancelledError as ce:
                # Provide clearer context for the common cancellation symptom the user saw
                raise RuntimeError(
                    "Initialization cancelled. This often means the server URL/path is incorrect or the server did not respond to the MCP initialize request."
                ) from ce
            except ConnectionResetError as cre:
                raise RuntimeError(
                    "Connection was reset by the server. Ensure the transport matches (use '/sse' for SSE servers) and that the server is running and reachable."
                ) from cre

            mcp_tools = await self.get_mcp_tools()

            self.logger.info(
                f"Successfully connected to server. Available tools: {[tool.name for tool in  mcp_tools]}"
            )
            return True
        except Exception as e:
            self.logger.error(f"Failed to connect to server at {server_url}: {e}")
            raise

    async def get_mcp_tools(self):
        try:
            self.logger.info("Requesting MCP tools from the server.")
            response = await self.session.list_tools()
            return response.tools
        except Exception as e:
            self.logger.error(f"Failed to get MCP tools: {str(e)}")
            raise Exception(f"Failed to get tools: {str(e)}")

    async def call_tool(self, tool_name: str, tool_args: dict):
        """Call a tool with the given name and arguments"""
        try:
            result = await self.session.call_tool(tool_name, tool_args)
            return result
        except Exception as e:
            self.logger.error(f"Failed to call tool: {str(e)}")
            raise Exception(f"Failed to call tool: {str(e)}")
        
    async def process_intent(self, intent: str):
        """Process an intent: prefer an LLM tool call; if none, return the LLM response.

        Returns the AI Responses API response object. If a tool is invoked, also returns
        the follow-up response after providing the tool result back to the model.
        """
        try:
            user_intent = {"role": "user", "content": intent}
            self.messages.append(user_intent) 
            self.results_log = {"intent": intent}
            # Also track for GenAI as typed Content
            try:
                self.genai_messages.append(
                    types.Content(role="user", parts=[types.Part(text=intent)])
                )
            except Exception:
                pass
            
            self.logger.info(f"Calling LLM: {self.llm_model}")
            payload = None
            if self.llm_model == "openai":
                payload = await self.call_openai()

            elif self.llm_model == "anthropic":
                payload = await self.call_anthropic()
            elif self.llm_model == "gemini":
                payload = await self.call_gemini()

            try:
                self.logger.info(f"Creating policy instance: {payload}")
                policy = requests.post(
                    f"{self.rapp}/create_policy",
                    json=payload,
                )
                self.logger.info(f"Policy instance status={policy.status_code}")
                
                return policy
            except Exception as e:
                self.logger.error(f"Error creating policy instance.: {str(e)}")
                raise  
            
        except Exception as e:
            self.logger.error(f"Error processing intent: {e}")
            raise

    async def cleanup(self):
        """Clean up resources (close streams & session)."""
        try:
            self.logger.info("Shuting down MCP connection.")
            await self.exit_stack.aclose()
        except Exception as e:
            self.logger.error(f"Error during cleanup: {str(e)}")

    async def call_openai(self):
        try:
            response = self.llm.responses.create(
                model="gpt-4o-mini",
                input=self.messages,
                tools=self.tools
            )
            for item in response.output:
                if item.type == "message":
                    assistant_message = {
                        "role": "assistant",
                        "content": item.content[0].text
                    }
                    self.messages.append(assistant_message)
                    self.logger.info("No tool_call found; returning text response.")
                    return assistant_message
                elif item.type == "function_call":
                    tool_name = item.name
                    tool_args = item.arguments
                    call_id = item.call_id
        
                    self.logger.info(f"Executing tool: {tool_name} with args: {tool_args}")
                    try:
                        tool_result = await self.session.call_tool(
                            tool_name, json.loads(tool_args)
                        )
                        self.logger.info(f"Tool result: {tool_result}")
                    except Exception as e:
                        error_msg = f"Tool execution failed for {tool_name}: {str(e)}"
                        self.logger.error(error_msg)
                        raise Exception(error_msg)

                    tool_output = None
                    try:
                        if getattr(tool_result, 'content', None):
                            block = tool_result.content[0]
                            tool_output = getattr(block, 'text', None) or str(block)
                    except Exception:
                        tool_output = "<no text content>"
                        raise Exception(tool_output)

                    self.messages.append({
                        "role": "user",
                        "content": f"Tool {tool_name} output (call_id={call_id}):\n{tool_output}"
                    })

                    self.logger.info("Setting policy type.")
                    final_response = self.llm.responses.create(
                        model="gpt-4o-mini",
                        input=self.messages,
                        tools=self.tools,
                        instructions=self.instructions
                    )
                    self.results_log.update({"tool_call": str(response), "type_definition": str(final_response)})
                    try:
                        save_results(self.results_log,self.result_file_path)
                    except Exception as e:
                        self.logger.error(f"Error saving results file: {e}")
                        raise
                    
                    for item in getattr(final_response, 'output'):
                            if getattr(item, 'content'):
                                body = (getattr(item, 'content')[0].text)
                                return json.loads(body)
        except Exception as e:
            self.logger.error(f"Error calling LLM: {e}")
            raise

    async def call_anthropic(self):
        try:
            response = self.llm.messages.create(
                    model="claude-sonnet-4-5",
                    max_tokens=1000,
                    messages=self.messages,
                    tools=self.tools,
                )
            
            if response.content[0].type == "text" and len(response.content) == 1:
                assistant_message = {
                    "role": "assistant",
                    "content": response.content[0].text,
                }
                self.messages.append(assistant_message)
                return assistant_message
            
            assistant_message = {
                "role": "assistant",
                "content": response.to_dict()["content"],
            }
            self.messages.append(assistant_message)  

            for content in response.content:
                if content.type == "tool_use":
                    tool_name = content.name
                    tool_args = content.input
                    tool_use_id = content.id
                    
                    self.logger.info(
                        f"Calling tool {tool_name} with args {tool_args}"
                    )
                    
                    try:
                        result = await self.session.call_tool(tool_name, tool_args)
                        self.logger.info(f"Tool {tool_name} result: {result}...")
                        
                        tool_output=result.content[0].text
                        self.messages.append({
                            "role": "user",
                            "content": [
                                {
                                    "type": "tool_result",
                                    "tool_use_id": tool_use_id,
                                    "content": tool_output,
                                },
                                {
                                    "type": "text",
                                    "text": self.instructions,
                                },
                            ],
                        })
                    except Exception as e:
                            self.logger.error(f"Error calling tool {tool_name}: {e}")
                            raise
                        
                    self.logger.info("Setting policy type.")

                    final_response = self.llm.messages.create(
                        model="claude-sonnet-4-5",
                        max_tokens=1000,
                        messages=self.messages,
                        tools=self.tools,
                    )
                    self.results_log.update({"tool_call": response, "type_definition": final_response})
                    save_results(self.results_log,self.result_file_path)
                    return final_response.content[0].text

        except Exception as e:
            self.logger.error(f"Error calling LLM: {e}")
            raise

    async def call_gemini(self):
        try:
            # Use dedicated typed contents for GenAI
            response = self.llm.models.generate_content(
                model="gemini-2.5-flash",
                contents=self.genai_messages,
                config=self.tools,
            )

            # Try to find a function call in any candidate/part
            function_call = None
            for cand in getattr(response, "candidates", []):
                for part in getattr(getattr(cand, "content", None), "parts", []) or []:
                    if getattr(part, "function_call", None):
                        function_call = part.function_call
                        break
                if function_call:
                    break

            if function_call:
                self.logger.info(f"Function to call: {function_call.name}")
                self.logger.info(f"Arguments: {function_call.args}")

                tool_name = function_call.name
                tool_args = function_call.args

                self.logger.info(
                    f"Calling tool {tool_name} with args {tool_args}"
                )

                try:
                    result = await self.session.call_tool(tool_name, tool_args)
                    self.logger.info(f"Tool {tool_name} result: {result}...")

                    tool_output = result.content[0].text

                    self.genai_messages.append(
                        types.Content(
                            role="user",
                            parts=[
                                types.Part.from_function_response(
                                    name=tool_name,
                                    response={"result": tool_output},
                                ),
                                types.Part(text=self.instructions),
                            ],
                        )
                    )

                    final_response = self.llm.models.generate_content(
                        model="gemini-2.5-flash",
                        contents=self.genai_messages,
                        config=self.tools,
                    )
                    self.results_log.update({"tool_call": response, "type_definition": final_response})
                    save_results(self.results_log,self.result_file_path)
                    
                    return getattr(final_response, "text", None)

                except Exception as e:
                    self.logger.error(f"Error calling tool {tool_name}: {e}")
                    raise
            
        except Exception as e:
            self.logger.error(f"Error calling LLM: {e}")
            raise
